{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnjqRAYBwEmy/KjHyt/e2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedakashyap7/Langchain-docs/blob/main/01_Model_I_O.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer [this documentation](https://python.langchain.com/docs/modules/model_io/)\n",
        "\n",
        "The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n",
        "- Prompts\n",
        "- Language Models\n",
        "- Output parsers"
      ],
      "metadata": {
        "id": "Ic_YYhGvqJ-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "Bs9PcnPCqT3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt templates\n",
        "- promt templates are predefined recipies for generating prompts for language models.\n",
        "- A template may include instructions, few-shot examples and specific context and questions appropriate for a given task.\n",
        "- use `PromptTemplate` class to create a template"
      ],
      "metadata": {
        "id": "QmdWkRzNqvRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkK62YRIqy8P",
        "outputId": "f261c77e-3d01-407e-d17b-7f536313db3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(adjective='funny', content='chickens')\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UHQrFmqksZ3s",
        "outputId": "34e98cd4-9256-4f7d-d13d-d09f4b582280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about chickens.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a way of creating a dynamic prompt with input variables\n",
        "# This will be used extensively in LLMChains in future modules\n",
        "\n",
        "prompt_template2 = PromptTemplate(\n",
        "    input_variables=['adjective','content'],\n",
        "    template = \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0guLOBC1s2lq",
        "outputId": "cd54e910-dcd7-4f63-af5f-565b6c01275c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['adjective', 'content'], template='Tell me a {adjective} joke about {content}.')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat prompt Template\n",
        "As we know there are chatmodels whose prompts need to be in a specified way.\n",
        "`ChatPromptTemplate` helps in creating prompts for a `ChatModel`"
      ],
      "metadata": {
        "id": "KjhMQ-qws3js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\"you are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\",\"Hello, how are you doing?\"),\n",
        "    (\"ai\",\"I'm doing well, thanks!\"),\n",
        "    (\"human\",\"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages = template.format_messages(name='bob',\n",
        "                                    user_input='what is your name?')\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8_jEKovwdVN",
        "outputId": "a4aad2ac-d5ec-427c-dba0-d87a4acfc04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='you are a helpful AI bot. Your name is bob.'),\n",
              " HumanMessage(content='Hello, how are you doing?'),\n",
              " AIMessage(content=\"I'm doing well, thanks!\"),\n",
              " HumanMessage(content='what is your name?')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ChatPromptTemplate.from_messages` accepts a variety of message representations.\n",
        "\n",
        "For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of` MessagePromptTemplate` or `BaseMessage`."
      ],
      "metadata": {
        "id": "tz8vHq65xg0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful assistant that re-writes the \\\n",
        "                            user's text to sound more upbeat\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "])\n",
        "\n",
        "template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUTghzTsyDM2",
        "outputId": "88c63ef0-d751-4624-961f-3e7bf9b7306a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['text'], messages=[SystemMessage(content=\"You are a helpful assistant that re-writes the                             user's text to sound more upbeat\"), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This provides you with a lot of flexibility in how you construct your chat prompts"
      ],
      "metadata": {
        "id": "DfFkwMMEzFCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to a Feature Store\n",
        "\n",
        "Langchain provides a way to call feature-store from inside a prompt template to retrive the values that are then formatted into the prompt"
      ],
      "metadata": {
        "id": "hzvDBywqzK6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feast\n",
        "Feast is an open source feature store framework"
      ],
      "metadata": {
        "id": "lN1zpzOZzZuu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CTHLPsEz4c_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}